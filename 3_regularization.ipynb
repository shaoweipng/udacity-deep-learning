{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmnist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's start with the logistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-b4636e904658>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 21.038488\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 12.6%\n",
      "Minibatch loss at step 500: 2.412453\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 1000: 1.783396\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 1500: 0.987118\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 2000: 0.861844\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 2500: 0.837882\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 3000: 0.778307\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 88.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The L2 regularization introduces a new meta parameter that should be tuned. Since I do not have any idea of what should be the right value for this meta parameter, I will plot the accuracy by the meta parameter value (in a logarithmic scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13eb59590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's see if the same technique will improve the prediction of the 1-layer neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-b4636e904658>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 642.789795\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 24.5%\n",
      "Minibatch loss at step 500: 195.031204\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 1000: 115.935646\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 1500: 68.408958\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2000: 41.275486\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2500: 25.232841\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 3000: 15.508781\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.3%\n",
      "Test accuracy: 92.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally something above 90%! I will also plot the final accuracy by the L2 parameter to find the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-20-ab3df16a8411>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEMCAYAAADDMN02AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4jfGmHbGEr8xxrQz/x9ZVpOzeDVjTgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106681710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-5989025b082e>:5: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 280.376953\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 33.1%\n",
      "Minibatch loss at step 2: 726.101929\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy: 37.6%\n",
      "Minibatch loss at step 4: 245.121506\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 53.5%\n",
      "Minibatch loss at step 6: 68.887024\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 62.1%\n",
      "Minibatch loss at step 8: 4.950906\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 10: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 12: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 18: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.4%\n",
      "Test accuracy: 68.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_bacthes = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_bacthes\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since there are far too much parameters and no regularization, the accuracy of the batches is 100%. The generalization capability is poor, as shown in the validation and test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-412275055f98>:5: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 414.711853\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 26.2%\n",
      "Minibatch loss at step 2: 1068.625854\n",
      "Minibatch accuracy: 41.4%\n",
      "Validation accuracy: 29.4%\n",
      "Minibatch loss at step 4: 581.495972\n",
      "Minibatch accuracy: 47.7%\n",
      "Validation accuracy: 52.3%\n",
      "Minibatch loss at step 6: 71.315178\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 62.3%\n",
      "Minibatch loss at step 8: 24.582695\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 10: 2.709989\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 65.0%\n",
      "Minibatch loss at step 12: 14.036414\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 14: 0.225260\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 16: 1.571457\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 63.9%\n",
      "Minibatch loss at step 18: 25.055798\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 66.3%\n",
      "Minibatch loss at step 20: 1.996138\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 66.3%\n",
      "Minibatch loss at step 22: 3.020426\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.3%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 28: 0.093843\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 30: 0.260764\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 32: 3.477716\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 40: 4.090987\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 42: 0.795705\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 44: 3.715480\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 48: 1.124765\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 52: 0.025020\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 54: 0.142309\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 58: 0.425192\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.4%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 64: 1.226134\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 66: 1.868681\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 74: 1.139145\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 76: 0.043769\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 78: 3.911308\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 84: 0.466411\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 90: 2.162521\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 94: 0.314977\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Test accuracy: 75.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first conclusion is that 100% of accuracy on the minibatches is more difficult achieved or to keep. As a result, the test accuracy is improved by 6%, the final net is more capable of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's do a first try with 2 layers. Note how the parameters are initialized, compared to the previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.272147\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 34.4%\n",
      "Minibatch loss at step 500: 0.930104\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1000: 0.904542\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 1500: 0.575127\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2000: 0.520965\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 2500: 0.531228\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 3000: 0.565390\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 3500: 0.573201\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 4000: 0.445847\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 4500: 0.444020\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 5000: 0.498980\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 5500: 0.493428\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 6000: 0.563357\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 6500: 0.390322\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 7000: 0.506404\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7500: 0.472213\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8000: 0.571431\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8500: 0.409382\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 9000: 0.470708\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 95.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is getting really good. Let's try one layer deeper with dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.427155\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 27.4%\n",
      "Minibatch loss at step 500: 0.363047\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1000: 0.466222\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 1500: 0.249981\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2000: 0.246187\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 2500: 0.279155\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 3000: 0.340918\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 3500: 0.344907\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 4000: 0.252765\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 4500: 0.248396\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 5000: 0.309714\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 5500: 0.205931\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 6000: 0.344032\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 6500: 0.167668\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7000: 0.291468\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 7500: 0.183530\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 8000: 0.275425\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 8500: 0.143154\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9000: 0.174426\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 9500: 0.191256\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 10000: 0.177660\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 10500: 0.156403\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 11000: 0.076319\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 11500: 0.141267\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 12000: 0.126884\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 12500: 0.100883\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 13000: 0.167142\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 13500: 0.077268\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 14000: 0.110395\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 14500: 0.094239\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 15000: 0.073499\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 15500: 0.083872\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 16000: 0.030450\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 16500: 0.058453\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 17000: 0.026212\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 17500: 0.013765\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 18000: 0.056984\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.9%\n",
      "Test accuracy: 96.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Huge! That's my best score on this dataset. I have also tried more parameters, but it does not help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "  drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "  logits = tf.matmul(drop3, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.80, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.644048\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 22.9%\n",
      "Minibatch loss at step 500: 0.505960\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 1000: 0.571871\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 1500: 0.519562\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 2000: 0.388242\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 2500: 0.469020\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 3000: 0.533019\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 3500: 0.550292\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 4000: 0.479638\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 4500: 0.430816\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5000: 0.413097\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 5500: 0.483560\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 6000: 0.562747\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 6500: 0.346888\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 7000: 0.523011\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 7500: 0.518974\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 8000: 0.692198\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 8500: 0.438252\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 9000: 0.436238\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 9500: 0.430096\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 10000: 0.506851\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 10500: 0.352449\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 11000: 0.386867\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 11500: 0.369807\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 12000: 0.622503\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 12500: 0.330038\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 13000: 0.437459\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 13500: 0.383894\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 14000: 0.422878\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 14500: 0.470360\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 15000: 0.400381\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 15500: 0.422781\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 16000: 0.276475\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 16500: 0.233879\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 17000: 0.289002\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 17500: 0.200542\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 18000: 0.277440\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 18500: 0.352895\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 19000: 0.293568\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 19500: 0.369922\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 20000: 0.426287\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 90.2%\n",
      "Test accuracy: 95.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
